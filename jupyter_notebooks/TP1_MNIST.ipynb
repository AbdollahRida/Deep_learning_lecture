{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step in deep learning with the MNIST data set\n",
    "\n",
    "The purpose of this notebook is to take its first step in deep learning by reproducing  the results given on the [MNIST site](http://yann.lecun.com/exdb/mnist/). In less than 3 minutes, you will build and train a fully connected neural network (NN) \n",
    "performing less than 1.5% error on the [MNIST database](http://yann.lecun.com/exdb/mnist/),\n",
    "and then, in less than 15 minutes, a convolutional neural network\n",
    "performing less than 1% error. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png\" \n",
    "alt=\"MNIST data\" width=\"240\" height=\"180\" border=\"1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to use [Keras: The Python Deep Learning library](https://keras.io/).  \n",
    "Why Keras, see for instance https://deepsense.ai/keras-or-pytorch/\n",
    "and https://github.com/ilkarman/DeepLearningFrameworks/ for a comparizon of different framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step toward reproducibility, the random generators seeds are fixed. \n",
    "For more details on the importance of reproducibility in machine learning see for instance the [MLTrain@RML ICML 2018 workshop](https://mltrain.cc/events/enabling-reproducibility-in-machine-learning-mltrainrml-icml-2018/). Unfortunately, it does not seem to be working very well yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephane/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)       \n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the MNIST data according to keras documentation https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: the input\n",
    "- normalize the data (so that the input $\\in$ [0,1])\n",
    "- reshape the input data so that: \n",
    "    - with `X_train` inputs are images: 28 x 28  matrices, adapted for convolutional NN,\n",
    "    - with `x_train` inputs are vectors of size 784, adapted for fully connected NN.  \n",
    "- check the size of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "x_train = X_train.reshape(60000, 784)  # reshape input from (28,28) to 784\n",
    "x_test = X_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some inputs to carry on checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c215cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_train[1], cmap='gray', interpolation='none')\n",
    "plt.title(\"Class {}\".format(y_train[1]))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_test[1], cmap='gray', interpolation='none')\n",
    "plt.title(\"Class {}\".format(y_test[1]))\n",
    "plt.show()\n",
    "\n",
    "x_train.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing: the output\n",
    "Now let's recode the output using a complete disjunctive coding in `Y_Train`. We have two ways of coding the outputs:\n",
    " - y_train a vector of 60 000 digits $\\in$ {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    " - Y_Train a binary matrix of size 60 000 by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10;\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "Y_train.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with fully connected neural networks (NN) \n",
    "They are also called multi layered perceptrons (MLP). \n",
    "In that case, \n",
    "the sequential model of keras is used mainly because its more simple (for a discusion on the two keras modes see for instance https://jovianlin.io/keras-models-sequential-vs-functional/).\n",
    "MLP have fully connected layers called `Dense` in Keras (https://keras.io/layers/core/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try\n",
    "Let's try to reproduce LeCun et al. 1998 reslts available on the [MNIST web site](http://yann.lecun.com/exdb/mnist/). With a\n",
    " 2-layer NN with 300 hidden units, the reported error rate is  4.7%.\n",
    "We begin with creating the architecture of the NN. \n",
    "- in that case the neural network is fully conected: the NN is sequential,\n",
    "- the first layer contained 300 neurons with `tanh` as an activation function,\n",
    "- the output layer contain 10 neurones (one per class) and the activation function is the `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN = Sequential()\n",
    "NN.add(Dense(units=300, activation='tanh', input_dim=784))\n",
    "NN.add(Dense(units=10, activation='softmax'))\n",
    "NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train the NN.  \n",
    "The first step here is to define the cost funtion (the loss) and the learning algorithm. \n",
    "A natural choice for the loss is the `categorical_crossentropy` well suited for multiclass classification. \n",
    "As a learning algorithm, let's began with the simple stochastic gradient descent optimizer, with default parameter initialization (that is a learning rate set at $0.01$).   \n",
    "The second step consist in fitting the NN to the data. \n",
    "By setting the `verbose` parameter to 0 (no outputs during training), the computing time is significantly reduced.\n",
    "This training phase may take about one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total computing time: 28.193730115890503\n"
     ]
    }
   ],
   "source": [
    "NN.compile(loss      = 'categorical_crossentropy',\n",
    "           optimizer = keras.optimizers.SGD(), \n",
    "           metrics   = ['accuracy'])\n",
    "t0=time.time()\n",
    "NN.fit(x_train, Y_train, epochs=10, verbose=0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of this NN is determined by measuring its error rate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step\n",
      "Test loss: 0.216 \n",
      "Test err:  6.12 %\n"
     ]
    }
   ],
   "source": [
    "score = NN.evaluate(x_test, Y_test)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second try\n",
    "Ok, let's improve our training a little bit by introducing some punch in our optimizer.\n",
    "To do so we add a momentum term (`momentum=0.9`) and some L2 penalty (`decay=1e-6`).\n",
    "\n",
    "This is done by  replacing the instruction  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`\n",
    "optimizer=keras.optimizers.SGD(), `  \n",
    "by  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`\n",
    "optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False), `\n",
    "\n",
    "For more details on optimizers, see for instance Sebastian Ruder's blog [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "[//]: # (https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/http://cs231n.github.io/neural-networks-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and resume training in the same way as we just did (it may take about one minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN1 = Sequential()\n",
    "NN1.add(Dense(units=300, activation='tanh', input_dim=784))\n",
    "NN1.add(Dense(units=10, activation='softmax'))\n",
    "NN1.compile(loss='categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False), \n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total computing time: 29.194577932357788\n",
      "10000/10000 [==============================] - 0s 26us/step\n",
      "Test loss: 0.070 \n",
      "Test err:  2.20 %\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "NN1.fit(x_train, Y_train, epochs=10, verbose=0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))\n",
    "\n",
    "score = NN1.evaluate(x_test, Y_test)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third try\n",
    "\n",
    "There is still room for some improvement. Let's try the same architecture with another optimizer.\n",
    "Try the `Adadelta` optimizer with default parameters(it may take about one minute).  \n",
    "To do so, replace the instruction  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False),`  \n",
    "by  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`\n",
    "              optimizer=keras.optimizers.Adadelta(), `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total computing time: 53.85512971878052\n",
      "10000/10000 [==============================] - 0s 27us/step\n",
      "Test loss: 0.060 \n",
      "Test err:  1.92 %\n"
     ]
    }
   ],
   "source": [
    "NN2 = Sequential()\n",
    "NN2.add(Dense(units=300, activation='tanh', input_dim=784))\n",
    "NN2.add(Dense(units=10, activation='softmax'))\n",
    "NN2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(), \n",
    "              metrics=['accuracy'])\n",
    "t0=time.time()\n",
    "NN2.fit(x_train, Y_train, epochs=10, verbose=0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))\n",
    "\n",
    "score = NN2.evaluate(x_test, Y_test)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth try\n",
    "\n",
    "In the MNIST leaderboad, with a \n",
    "3-layer NN, 500+300 HU, softmax, cross entropy, weight decay, Hinton, reproted in 2005 an error rate of 1.53%. \t \n",
    "For fun we can use a ReLU activation function. The training phase takes about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 545,810\n",
      "Trainable params: 545,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "total computing time: 203.28213500976562\n",
      "Test loss: 0.073 \n",
      "Test err:  1.46 %\n"
     ]
    }
   ],
   "source": [
    "NN3 = Sequential()\n",
    "NN3.add(Dense(units=500, activation='relu', input_dim=784))\n",
    "NN3.add(Dense(units=300, activation='tanh'))\n",
    "NN3.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "NN3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "NN3.summary()\n",
    "              \n",
    "t0=time.time()\n",
    "NN3.fit(x_train, Y_train, epochs=20, verbose = 0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))\n",
    "\n",
    "score = NN3.evaluate(x_test, Y_test, verbose=0)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's continue with Convolutional neural networks (CNN) \n",
    "\n",
    "In the MNIST leaderboad, LeCun et al. 1998 reported with LeNet-5 an error rate of 0.95%.\n",
    "LeNet-5 is a convolutional neural network.\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-16-at-11.34.51-AM.png\" alt=\"MNIST data\" width=\"360\" height=\"270\" border=\"5\" />\n",
    "Convolution used to process images are 2d convolutions taking tensors as input and output.\n",
    "To fit into the first 2d convolutional layer, the input have to be recast as tensor of dimension $28 \\times 28 \\times 1$. This can be perform by using a reshape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try\n",
    "With a simplified version of LeNet-5 and with ReLU instead of hyperbolic tangent as activation function. It takes about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total computing time: 140.80635786056519\n",
      "10000/10000 [==============================] - 1s 113us/step\n",
      "Test loss: 0.036 \n",
      "Test err:  0.93 %\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "CNN = Sequential()\n",
    "\n",
    "CNN.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28,28,1)))\n",
    "CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN.add(Flatten())\n",
    "CNN.add(Dense(120, activation='relu'))\n",
    "CNN.add(Dense(84, activation='relu'))\n",
    "CNN.add(Dense(10, activation='softmax'))\n",
    "\n",
    "CNN.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "\n",
    "t0=time.time()\n",
    "CNN.fit(X_train, Y_train, epochs=10, verbose=0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))\n",
    "\n",
    "score = CNN.evaluate(X_test, Y_test)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second try \n",
    "With a CNN slightly more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 22, 22, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               991360    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,011,978\n",
      "Trainable params: 1,011,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN = Sequential()\n",
    "CNN.add(Conv2D(32, kernel_size=(5, 5),\n",
    "           activation='relu',\n",
    "          input_shape=(28,28,1)))\n",
    "CNN.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN.add(Dropout(0.25))\n",
    "CNN.add(Flatten())\n",
    "CNN.add(Dense(128, activation='relu'))\n",
    "CNN.add(Dropout(0.5))\n",
    "CNN.add(Dense(10, activation='softmax'))\n",
    "CNN.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we proceed to the training in the same way, using `Adadelta` as optimizer. It may take about 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total computing time: 761.9349110126495\n",
      "10000/10000 [==============================] - 4s 417us/step\n",
      "Test loss: 0.024 \n",
      "Test err:  0.63 %\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "CNN.fit(X_train, Y_train, epochs=10, verbose=0)\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))\n",
    "\n",
    "score = CNN.evaluate(X_test, Y_test)  \n",
    "err = 100*(1-np.array(score))\n",
    "print('Test loss: %4.3f '%score[0])\n",
    "print('Test err:  %4.2f %%'% err[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's sumarize\n",
    "| Architecture  |Size&nbsp;&nbsp;| Optimizer  |CPU Time |GPU Time | Error   |\n",
    "|:------------- |---------------:|:----------:|:-------:|:-------:|:-------:|\n",
    "| NN 300 HU     |   238,510      | SGD        | 28&nbsp;| 20&nbsp;| 6.12%   |\n",
    "| NN 300 HU     |   238,510      | SGD+mom+wd | 29&nbsp;| 22&nbsp;| 2.20%   |\n",
    "| NN 300 HU     |   238,510      | Adadelta   | 54&nbsp;| 54&nbsp;| 1.92%   |\n",
    "| NN 500-300 HU |   545,810      | Adadelta   |203&nbsp;| 64&nbsp;| 1.46%   |\n",
    "| CNN My_LeNet-5|    44,426      | Adadelta   |141&nbsp;| 56&nbsp;| 0.93%   |\n",
    "| CNN Larger    |  1,011,978     | Adadelta   |762&nbsp;| 71&nbsp;| 0.63%   |\n",
    "\n",
    "The GPU used is a NVIDIA GeForce GTX 1080 Ti (thanks to [Cyprien Ruffino](https://github.com/cyprienruffino)) and the CPU is the one of a MacBookPro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn: \n",
    "\n",
    "Do your best with the [Fashion-MNIST data set](https://github.com/zalandoresearch/fashion-mnist) with a budget of 1 minute, 5 minutes and 15 minutes, and compare with the state of the art (see for instance https://www.kaggle.com/pavansanagapati/fashion-mnist-cnn-model-with-tensorflow-keras) and do better than the Google autoML https://www.statworx.com/de/blog/a-performance-benchmark-of-google-automl-vision-using-fashion-mnist/.\n",
    "\n",
    "<img src=\"https://markusthill.github.io/images/2017-10-12-zalandos-fashion-mnist-dataset/zalando10x10.jpeg\" alt=\"MNIST data\" width=\"360\" height=\"270\" border=\"5\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
