{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning alone, without any framework (on the MNIST data set)\n",
    "\n",
    "The purpose of this notebook is to train a 3 layered neural network without any framework.\n",
    "Propagation takes 3 lines in python\n",
    "and back propagation takes 4 lines.\n",
    "\n",
    "By reproducing  the results given on the [MNIST site](http://yann.lecun.com/exdb/mnist/). In less than 2 minutes, you will build and train a fully connected neural network (NN) \n",
    "performing about 2% error on the [MNIST database](http://yann.lecun.com/exdb/mnist/),\n",
    "\n",
    "Let's begin with loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephane/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)       \n",
    "\n",
    "from keras.datasets import mnist\n",
    "#from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "x_train = X_train.reshape(60000, 784)  # reshape input from (28,28) to 784\n",
    "x_test = X_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "num_classes = 10;\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining an activation function (see for instance https://github.com/mlelarge/dataflowr/tree/master/Notebooks)  \n",
    "and for the cross entropy loss  (see for instance https://deepnotes.io/softmax-crossentropy or \n",
    "https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/),  \n",
    "together with their derivative or grandient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(object):\n",
    "    def forward(x):\n",
    "        return np.maximum(0, x)   \n",
    "    def backward(x):\n",
    "        return np.maximum(0, np.sign(x))\n",
    "    \n",
    "class MyCrossEntropy(object):\n",
    "    def Cost(y,yt):\n",
    "        exps = np.exp(y)            \n",
    "        exps /= np.sum(exps, axis=0)   # normalize\n",
    "        return -np.sum(yt*np.log(exps),axis=0),exps\n",
    "    def Grad_Cost(y,yt):\n",
    "        y /= np.sum(y, axis=0)   # normalize\n",
    "        return -np.sum(yt*np.log(y),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = x_train.shape\n",
    "batch_size = 128\n",
    "\n",
    "n1 = 300;\n",
    "sig = 0.05\n",
    "W1 = sig*np.random.randn(n1,p+1)\n",
    "W2 = sig*np.random.randn(10,n1+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagation is performed in paralel on all the exemples taken within the mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.choice(np.arange(n),batch_size, replace=False)\n",
    "\n",
    "x  = x_train[ind,]\n",
    "a1 = W1@np.r_[x.T, np.ones((1,batch_size))]\n",
    "h1 = MyReLU.forward(a1)\n",
    "a2 = W2@np.r_[h1, np.ones((1,batch_size))]\n",
    "y = MyReLU.forward(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784)\n",
      "(300, 785)\n",
      "(300, 128)\n",
      "(300, 128)\n",
      "(10, 301)\n",
      "(10, 128)\n",
      "(10, 128)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(W1.shape)\n",
    "print(a1.shape)\n",
    "print(h1.shape)\n",
    "print(W2.shape)\n",
    "print(a2.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298.89242651165546\n"
     ]
    }
   ],
   "source": [
    "err, exps = MyCrossEntropy.Cost(y,Y_train[ind,].T)\n",
    "errT = np.sum(err)\n",
    "print(errT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation with L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 0.0001;\n",
    "\n",
    "dJ_dy  = exps - Y_train[ind,].T\n",
    "dJ_da2 = dJ_dy * MyReLU.backward(a2)\n",
    "gradW2 = dJ_da2@np.r_[h1, np.ones((1,batch_size))].T + lambd*W2\n",
    "dJ_da1 = W2[:,0:n1].T@dJ_da2 * MyReLU.backward(a1)\n",
    "gradW1 = dJ_da1@np.r_[x.T, np.ones((1,batch_size))].T + lambd*W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 128)\n",
      "(10, 128)\n",
      "(10, 128)\n",
      "(10, 301)\n",
      "(300, 128)\n",
      "(300, 785)\n"
     ]
    }
   ],
   "source": [
    "print(exps.shape)\n",
    "print(dJ_dy.shape)\n",
    "print(dJ_da2.shape)\n",
    "print(gradW2.shape)\n",
    "print(dJ_da1.shape)\n",
    "print(gradW1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize = 0.001/batch_size;\n",
    "W1 = W1 - stepsize*gradW1\n",
    "W2 = W2 - stepsize*gradW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop.\n",
    "Before, some initialization are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23438\n"
     ]
    }
   ],
   "source": [
    "n,p = x_train.shape\n",
    "batch_size = 128\n",
    "\n",
    "n1 = 300;\n",
    "sig = 0.05\n",
    "W1 = sig*np.random.randn(n1,p+1)\n",
    "W2 = sig*np.random.randn(10,n1+1)\n",
    "\n",
    "lambd = 0.01;\n",
    "\n",
    "n_epoch = 50\n",
    "nb_ite_max = int(np.round(n_epoch*n/batch_size));\n",
    "print(nb_ite_max)\n",
    "\n",
    "stepsize = 0.1/batch_size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the initial error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.54\n"
     ]
    }
   ],
   "source": [
    "nt,p = x_test.shape\n",
    "\n",
    "a1 = W1@np.r_[x_test.T, np.ones((1,nt))]\n",
    "h1 = MyReLU.forward(a1)\n",
    "y = W2@np.r_[h1, np.ones((1,nt))]\n",
    "\n",
    "winner = y.max(0).reshape((nt,1))\n",
    "yp = (y == np.outer(np.ones((10,1)),winner)).astype(int)\n",
    "print(100*np.sum(np.abs(yp - Y_test.T))/nt/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about two minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total computing time: 112.7305588722229\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "for i in range(nb_ite_max):\n",
    "#                                                                  RANDOMLY PICKING THE MINIBACH    \n",
    "    ind = np.random.choice(np.arange(n),batch_size, replace=False)\n",
    "#                                                                  PROPAGATION\n",
    "    x  = x_train[ind,]\n",
    "    a1 = W1@np.r_[x.T, np.ones((1,batch_size))]\n",
    "    h1 = MyReLU.forward(a1)\n",
    "    y  = W2@np.r_[h1, np.ones((1,batch_size))]\n",
    "#                                                                  COST    \n",
    "    err, exps = MyCrossEntropy.Cost(y,Y_train[ind,].T)\n",
    "    errT = np.sum(err)\n",
    "#                                                                  BACK PROPAGATION    \n",
    "    dJ_da2 = exps - Y_train[ind,].T\n",
    "    gradW2 = dJ_da2@np.r_[h1, np.ones((1,batch_size))].T + lambd*W2\n",
    "    dJ_da1 = W2[:,0:n1].T@dJ_da2 * MyReLU.backward(a1)\n",
    "    gradW1 = dJ_da1@np.r_[x.T, np.ones((1,batch_size))].T + lambd*W1\n",
    "#                                                                  GRADIENT UPDATE\n",
    "    W1 = W1 - stepsize*gradW1\n",
    "    W2 = W2 - stepsize*gradW2\n",
    "\n",
    " #   print(errT)\n",
    "print('total computing time: '+str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the final error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate:  2.01 %\n"
     ]
    }
   ],
   "source": [
    "a1 = W1@np.r_[x_test.T, np.ones((1,nt))]\n",
    "h1 = MyReLU.forward(a1)\n",
    "y = W2@np.r_[h1, np.ones((1,nt))]\n",
    "\n",
    "winner = y.max(0).reshape((nt,1))\n",
    "yp = (y == np.outer(np.ones((10,1)),winner)).astype(int)\n",
    "err_rate = 100*np.sum(np.abs(yp - Y_test.T))/nt/2\n",
    "print('Test error rate:  %4.2f %%'% err_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Of course, it is far from being finished, a many imporvement are waiting to be coded. \n",
    "Still, we hope that these few lines give an good idea of the nature of deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
